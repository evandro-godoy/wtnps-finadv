# Deep Reinforcement Learning (DRL) - Integra√ß√£o WTNPS Trade

## üìã Vis√£o Geral

Esta integra√ß√£o adiciona capacidades de **Deep Reinforcement Learning** ao framework `wtnps-trade`, permitindo que agentes DDQN (Double Deep Q-Network) aprendam estrat√©gias de trading diretamente atrav√©s de recompensas baseadas em performance.

A implementa√ß√£o √© **totalmente compat√≠vel** com a arquitetura modular existente:
- ‚úÖ Usa `configs/main.yaml` para configura√ß√£o
- ‚úÖ Herda de `BaseStrategy` 
- ‚úÖ Funciona com `SimulationEngine` e `live_trader.py`
- ‚úÖ Consome dados via `MetaTraderProvider` ou `YFinanceProvider`

---

## üèóÔ∏è Arquitetura

### Componentes Criados

```
src/
‚îú‚îÄ‚îÄ environments/
‚îÇ   ‚îî‚îÄ‚îÄ trading_env.py         # Ambiente de treinamento customizado
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îî‚îÄ‚îÄ drl_agent.py           # Agente DDQN com replay buffer
‚îú‚îÄ‚îÄ strategies/
‚îÇ   ‚îî‚îÄ‚îÄ drl_strategy.py        # Interface de infer√™ncia (herda BaseStrategy)

train_drl_model.py              # Script de treinamento principal
```

### Fluxo de Dados

```
TREINAMENTO:
configs/main.yaml ‚Üí TradingEnv ‚Üí MetaTraderProvider ‚Üí Dados Hist√≥ricos
                       ‚Üì
                   DDQNAgent (DDQN) ‚Üí Treina Q-Network
                       ‚Üì
                   Modelo Salvo (models/WDO$_DRL_prod_drl.keras)

INFER√äNCIA:
SimulationEngine ‚Üí DRLStrategy.load() ‚Üí Q-Network
       ‚Üì
Estado (market_features + position) ‚Üí Q-Network ‚Üí A√ß√£o (0/1/2)
       ‚Üì
SetupAnalyzer (opcional) ‚Üí Decis√£o Final
```

---

## üéØ Como Funciona

### 1. Ambiente de Trading (`TradingEnv`)

O ambiente **n√£o** herda de `gym.Env` (customizado para nosso projeto).

#### Estado (State)
```python
State = [market_features, position_feature]
```
- **Market features** (6 dimens√µes):
  - `ema_9`: Exponential Moving Average (9 per√≠odos, normalizada)
  - `sma_20`: Simple Moving Average (20 per√≠odos, normalizada)
  - `sma_200`: Simple Moving Average (200 per√≠odos, normalizada)
  - `dist_sma_20`: Dist√¢ncia normalizada do pre√ßo para SMA 20
  - `dist_sma_200`: Dist√¢ncia normalizada do pre√ßo para SMA 200
  - `atr`: Average True Range normalizado (volatilidade)

- **Position feature** (3 dimens√µes, one-hot):
  - `[1,0,0]` = Venda (short)
  - `[0,1,0]` = Hold (cash)
  - `[0,0,1]` = Compra (long)

**Total: 9 dimens√µes**

#### A√ß√µes (Actions)
- `0`: VENDA (short)
- `1`: HOLD (neutro)
- `2`: COMPRA (long)

#### Recompensa (Reward)
Baseada em **log returns do portf√≥lio** (FinancialTradingasaGameDRL.pdf):

```python
PnL = (pre√ßo_atual - pre√ßo_anterior) / pre√ßo_anterior  # Ajustado por posi√ß√£o
Custo = transaction_cost_pct se mudou posi√ß√£o
Reward = log( (1 + PnL - Custo) )
```

---

### 2. Agente DDQN (`DDQNAgent`)

Implementa **Double Deep Q-Learning** com:
- **Online Network**: Atualizada ap√≥s cada epis√≥dio (batch training)
- **Target Network**: Atualizada a cada `tau` steps (estabilidade)
- **Replay Buffer**: Armazena at√© 1M experi√™ncias
- **Epsilon-Greedy**: Explora√ß√£o decai de 1.0 ‚Üí 0.01

#### Arquitetura da Q-Network
```
Input (state_dim=9) 
    ‚Üí Dense(256, relu) 
    ‚Üí Dense(256, relu) 
    ‚Üí Dropout(0.1)
    ‚Üí Output(3)  # Q-values para cada a√ß√£o
```

---

### 3. Estrat√©gia DRL (`DRLStrategy`)

Implementa a interface `BaseStrategy` para compatibilidade com o engine:

```python
# Carregamento (usado por SimulationEngine)
model = DRLStrategy.load("models/WDO$_DRL_prod")

# Infer√™ncia (engine constr√≥i o estado internamente)
q_values = model.predict(state_vector)
action = np.argmax(q_values)  # 0, 1, ou 2
```

**Diferencial**: O `SimulationEngine` rastreia a posi√ß√£o atual (`self.asset_positions`) para construir o `position_feature` corretamente a cada ciclo.

---

## üöÄ Guia de Uso

### Passo 1: Configura√ß√£o

Adicione a estrat√©gia DRL ao seu ativo em `configs/main.yaml`:

```yaml
assets:
  - ticker: "WDO$"
    enabled: true
    
    strategies:
      # Estrat√©gia LSTM (tradicional)
      - name: "LSTMStrategy"
        module: "lstm"
        provider: "MetaTrader5"
        data:
          start_date: "2023-01-01"
          end_date: "2025-10-31"
          timeframe_model: "D1"
        training_trading_rules:
          initial_capital: 5000
          stop_loss_pct: 0.01
          take_profit_pct: 0.03
      
      # Estrat√©gia DRL (nova!)
      - name: "DRLStrategy"
        module: "drl_strategy"
        provider: "MetaTrader5"
        data:
          start_date: "2023-01-01"
          end_date: "2025-10-31"
          timeframe_model: "D1"
        training_trading_rules:
          initial_capital: 5000
          stop_loss_pct: 0.01  # Usado como custo de transa√ß√£o no ambiente
          take_profit_pct: 0.03
    
    # Regras gerais de live trading (aplicadas a todas as estrat√©gias)
    trading_rules:
      initial_capital: 5000
      stop_loss_ticks: 20
      stop_loss_pct: 0.01
      take_profit_pct: 0.03
    
    live_trading:
      enabled: false  # Habilite ap√≥s testes
      ticker_order: "WDOX25"
      timeframe_str: "M5"
      execution_mode: "suggest"
      trade_volume: 1.0
    
    setup: []  # DRL n√£o usa setups de TA (opcional)
```

**Nota**: Cada ticker pode ter **m√∫ltiplas estrat√©gias**. A primeira estrat√©gia da lista √© usada para live trading.

### Passo 2: Treinamento

Execute o script de treinamento:

```powershell
poetry run python train_drl_model.py
```

O script ir√°:
1. Listar ativos dispon√≠veis no config
2. Pedir o ticker (ex: `WDO$_DRL`)
3. Pedir n√∫mero de epis√≥dios (default: 1000)
4. Treinar o agente DDQN
5. Salvar o modelo em `models/WDO$_DRL_prod_drl.keras`

**Exemplo de sa√≠da**:
```
Episode  100/1000 | Reward(100):  -0.0234 | Reward(10):  -0.0189 | Epsilon: 0.6000
Episode  200/1000 | Reward(100):  -0.0112 | Reward(10):   0.0045 | Epsilon: 0.2000
Episode  500/1000 | Reward(100):   0.0231 | Reward(10):   0.0412 | Epsilon: 0.0100
```

**‚è±Ô∏è Tempo estimado**: ~10-30 minutos (depende do hardware e n√∫mero de epis√≥dios)

### Passo 3: Teste (Simula√ß√£o)

Use o `SimulationEngine` para testar o modelo treinado:

```python
# Em um notebook ou script
from src.simulation.engine import SimulationEngine
from datetime import datetime

engine = SimulationEngine()

result = engine.run_simulation_cycle(
    asset_symbol="WDO$",
    strategy_name="DRLStrategy",  # Especifica qual estrat√©gia usar
    timeframe_str="D1",
    target_datetime_local=datetime(2025, 10, 15, 12, 0)
)

print(result)
# {
#   'ai_signal': 'COMPRA',
#   'ai_signal_code': 2,
#   'setup_valid': True,
#   'final_decision': 'COMPRA',
#   'current_price': 123456.0,
#   ...
# }
```

**Ou use o notebook existente**:
- `notebooks/simulation/engine_simulation_single_cycle.ipynb`

### Passo 4: Live Trading (Opcional)

Configure `live_trading.enabled: true` no config e execute:

```powershell
poetry run python src/live_trader.py
```

‚ö†Ô∏è **ATEN√á√ÉO**: Teste EXTENSIVAMENTE em modo `suggest` antes de usar `execute`!

---

## üìä Monitoramento e M√©tricas

Durante o treinamento, o agente registra:
- **Recompensa por epis√≥dio**: `agent.rewards_history`
- **Epsilon (explora√ß√£o)**: `agent.epsilon_history`
- **Loss da Q-Network**: `agent.losses`
- **Steps por epis√≥dio**: `agent.steps_per_episode`

Voc√™ pode estender `train_drl_model.py` para salvar essas m√©tricas em CSV ou usar TensorBoard.

---

## üîß Hiperpar√¢metros

Hiperpar√¢metros padr√£o (ajust√°veis em `train_drl_model.py`):

```python
{
    'learning_rate': 0.0001,
    'gamma': 0.99,                      # Fator de desconto
    'epsilon_start': 1.0,
    'epsilon_end': 0.01,
    'epsilon_decay_steps': 250,         # Epis√≥dios para decay linear
    'epsilon_exponential_decay': 0.99,
    'replay_capacity': 1_000_000,
    'architecture': (256, 256),         # Camadas ocultas
    'l2_reg': 1e-6,
    'tau': 100,                         # Target network update freq
    'batch_size': 4096
}
```

### Dicas de Ajuste
- **Overfitting**: Aumente `l2_reg`, reduza `architecture`
- **Underfitting**: Aumente `architecture`, `num_episodes`
- **Explora√ß√£o insuficiente**: Aumente `epsilon_decay_steps`
- **Instabilidade**: Reduza `learning_rate`, aumente `tau`

---

## üß™ Extens√µes Poss√≠veis

### 1. DRQN (Deep Recurrent Q-Network)
Adicione camadas LSTM √† Q-Network para mem√≥ria temporal:

```python
# Em drl_agent.py
from tensorflow.keras.layers import LSTM

layers.append(LSTM(128, return_sequences=False))
```

Ajuste `TradingEnv` para retornar sequ√™ncias de estados.

### 2. Prioritized Experience Replay
Priorize experi√™ncias com maior TD-error:

```python
# Em ReplayBuffer
def sample(self, batch_size):
    priorities = np.abs(self.td_errors) + 1e-6
    probs = priorities / priorities.sum()
    indices = np.random.choice(len(self), batch_size, p=probs)
    ...
```

### 3. Multi-Asset DRL
Treine um √∫nico agente para m√∫ltiplos ativos:
- Adicione `asset_id` ao estado
- Compartilhe Q-Network entre ativos

### 4. Dueling DQN
Separe value e advantage streams na Q-Network:

```python
# Camada de sa√≠da dupla
value = Dense(1)(x)
advantage = Dense(num_actions)(x)
q_values = value + (advantage - tf.reduce_mean(advantage))
```

---

## üêõ Troubleshooting

### Problema: "Modelo DRL n√£o encontrado"
**Solu√ß√£o**: Execute `train_drl_model.py` primeiro.

### Problema: Recompensas sempre negativas
**Causas**:
- Custos de transa√ß√£o muito altos (`stop_loss_pct`)
- Epsilon muito alto (ainda explorando)
- Dados insuficientes

**Solu√ß√£o**: Reduza `stop_loss_pct`, aumente epis√≥dios de treino.

### Problema: "KeyError: 'WDO$_DRL'"
**Solu√ß√£o**: Verifique se o ticker est√° em `configs/main.yaml` e `enabled: true`.

### Problema: Q-values explodem (NaN/Inf)
**Causas**: Learning rate muito alto, gradientes explodem

**Solu√ß√£o**: 
- Reduza `learning_rate` (ex: 0.00001)
- Adicione gradient clipping:
```python
optimizer = Adam(learning_rate=lr, clipnorm=1.0)
```

---

## üìö Refer√™ncias

1. **FinancialTradingasaGameDRL.pdf**: Framework te√≥rico (State, Reward, log returns)
2. **04_q_learning_for_trading.ipynb**: Implementa√ß√£o base do DDQN
3. **Playing Atari with Deep Reinforcement Learning** (Mnih et al., 2013): DQN original
4. **Human-level control through deep RL** (Mnih et al., 2015): Target network
5. **Deep Reinforcement Learning with Double Q-learning** (van Hasselt, 2015): DDQN

---

## ‚úÖ Checklist de Valida√ß√£o

Antes de usar em produ√ß√£o:

- [ ] Modelo treinado em pelo menos 1000 epis√≥dios
- [ ] Recompensa m√©dia positiva nos √∫ltimos 100 epis√≥dios
- [ ] Testado em `SimulationEngine` com dados out-of-sample
- [ ] Comparado com estrat√©gia buy-and-hold
- [ ] Verificado em diferentes condi√ß√µes de mercado (alta, baixa, lateral)
- [ ] Testado em modo `suggest` por pelo menos 1 semana
- [ ] Documentado hiperpar√¢metros e resultados de treino

---

## ü§ù Contribuindo

Para adicionar melhorias ao m√≥dulo DRL:

1. Implemente a feature em `src/agents/` ou `src/environments/`
2. Mantenha compatibilidade com `BaseStrategy`
3. Atualize este README
4. Adicione testes em `tests/`

---

## üìß Contato

Para d√∫vidas sobre a integra√ß√£o DRL, consulte:
- `README.md` principal do projeto
- `.github/copilot-instructions.md`
- Issues no reposit√≥rio

---

**Status**: ‚úÖ Produ√ß√£o | **Vers√£o**: 1.0 | **Data**: 2025-11-07
